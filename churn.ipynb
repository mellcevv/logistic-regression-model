{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d094142",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('your-file-path.csv')\n",
    "\n",
    "y = X.churn_probability\n",
    "# y represents the dependent variable we wanted to predict  \n",
    "# in this case we are wanting to predict churn probabilities of customers from some telecom company          \n",
    " \n",
    "X.drop(['churn_probability'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfae017",
   "metadata": {},
   "source": [
    "IF YOU HAVE COLUMNS THAT HAVE HIGH MISSING VALUES, YOU CAN DELETE THEM BY USING FOLLOWING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = len(X) * .80 #you can adjust the limit by chancing allowed percentage of missing values\n",
    "X = X.dropna(thresh=limit, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c91749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting datetime columns\n",
    "date_cols = list(X.select_dtypes(include=['object', 'category']).columns)\n",
    "\n",
    "\n",
    "# adjust the threshold for low_cardinality columns as needed to capture all of your categorical variables.\n",
    "low_cardinality_cols = [col for col in X.columns if X[col].nunique() < 3]\n",
    "\n",
    "# categorical cols also includes datetime values, remove them\n",
    "cat_cols = [col for col in  low_cardinality_cols if col not in date_cols]\n",
    "\n",
    "num_cols = [col for col in X.columns if col not in date_cols + cat_cols]\n",
    "\n",
    "# not do any preprocessing in id, because we predict each customer's churn prob\n",
    "num_cols.remove('id')\n",
    "\n",
    "\n",
    "# you can make adjustments for checking if you capture all the columns\n",
    "#print('All Columns:', X.columns)\n",
    "#print('Numerical Columns:', num_cols)\n",
    "#print('Categorical Columns:',cat_cols)\n",
    "#print('Date Columns:',(date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters can be changed for obtaining more accurate predictions\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "date_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ('date', date_transformer, date_cols)])\n",
    "\n",
    "\n",
    "# full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs', max_iter=10000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e293ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f41145",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new dataframe which contains customer ids and churn probabilities\n",
    "\n",
    "pred = pd.DataFrame(pred,columns=['churn_probability'])\n",
    "ID = pd.DataFrame(X_test.index,columns=['id'])\n",
    "sub = pd.concat([ID,pred],axis=1)\n",
    "sub.set_index('id',inplace=True)\n",
    "\n",
    "\n",
    "# converting data frame into csv file\n",
    "sub.to_csv(f\"submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
